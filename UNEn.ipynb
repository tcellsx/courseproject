{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4fb2c189-1599-45bf-a893-6128c9fe623d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def neighborhood_lower_bound(graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    lower_bounds = {}\n",
    "\n",
    "    k = 2\n",
    "    max_iterations = 100000  # Set maximum iterations\n",
    "\n",
    "    Y = {}\n",
    "    S_un = {}\n",
    "    nVisited = {}\n",
    "    finished = {}\n",
    "\n",
    "    for s in nodes:\n",
    "        degree_s = graph.degree(s)\n",
    "        Y[(1, s)] = degree_s\n",
    "        S_un[s] = degree_s\n",
    "        nVisited[s] = degree_s + 1\n",
    "        finished[s] = False\n",
    "\n",
    "    nFinished = 0\n",
    "\n",
    "    while nFinished < n and k <= max_iterations:\n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "            if k == 2:\n",
    "                Y[(k, s)] = sum(Y.get((k-1, w), 0) for w in graph.neighbors(s)) - graph.degree(s)\n",
    "            else:\n",
    "                Y[(k, s)] = sum(Y.get((k-1, w), 0) for w in graph.neighbors(s)) - Y.get((k-2, s), 0) * (graph.degree(s) - 1)\n",
    "        \n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "            y_k_minus_2 = Y.get((k-2, s), 0)\n",
    "            y_k_minus_1 = Y.get((k-1, s), 0)\n",
    "            nVisited[s] += y_k_minus_1\n",
    "            \n",
    "            if nVisited[s] < n:\n",
    "                S_un[s] += k * y_k_minus_1\n",
    "            else:\n",
    "                S_un[s] += k *(n - (nVisited[s] - y_k_minus_1))\n",
    "                nFinished += 1\n",
    "                finished[s] = True\n",
    "            Y[(k-2, s)] = y_k_minus_1\n",
    "            Y[(k-1, s)] = Y[(k, s)]\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    for v in nodes:\n",
    "        lower_bounds[v] = S_un[v] / (n - 1)\n",
    "\n",
    "    return lower_bounds\n",
    "\n",
    "def updateBoundsBFSCut(v, graph, x):\n",
    "    \"\"\"\n",
    "    Calculate the lower bound of the cut size that separates vertex v.\n",
    "\n",
    "    Parameters:\n",
    "        v: Starting vertex.\n",
    "        graph: networkx DiGraph. Nodes must have 'r' attribute.\n",
    "        Farn: Dictionary that stores the lower bounds (must be initialized).\n",
    "        Top: List of top k nodes (must be initialized).\n",
    "        x: A threshold.\n",
    "\n",
    "    Returns:\n",
    "        If the cut value exceeds x, return +âˆž; otherwise, return the calculated cut value or the current Farn[v] if unchanged.\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    Q = [(0, v)]  # Priority queue to track BFS (distance, node)\n",
    "    heapq.heapify(Q)\n",
    "    visited = {v}\n",
    "    d = 0\n",
    "    S = 0\n",
    "    y = graph.degree(v) - 1\n",
    "    nd = 1\n",
    "\n",
    "    while Q:\n",
    "        dist, u = heapq.heappop(Q)\n",
    "        if dist > d:\n",
    "            d += 1\n",
    "            # LCUT calculation\n",
    "            LCUT = ((d + 2) * (n - nd) + S - y) / (n - 1)\n",
    "\n",
    "            if LCUT >= x:\n",
    "                return float('inf')\n",
    "            y = 0  # Reset y\n",
    "\n",
    "        for w in graph.neighbors(u):\n",
    "            if w not in visited:\n",
    "                visited.add(w)\n",
    "                dist_vw = nx.shortest_path_length(graph, v, w)  # Assume a shortest path exists\n",
    "                heapq.heappush(Q, (dist_vw, w))\n",
    "                S += dist_vw  # Distance d(v, w)\n",
    "                y += graph.degree(w)\n",
    "                nd += 1\n",
    "            else:\n",
    "                LCUT += 1 / (n - 1)\n",
    "\n",
    "    # Final calculation\n",
    "    LCUT_final = S / (n - 1)\n",
    "\n",
    "    return LCUT_final\n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds...\")\n",
    "\n",
    "    initial_lower_bounds = neighborhood_lower_bound(graph)\n",
    "        k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    for node, farness in k_lowest_farness:\n",
    "        print(f\"Node: {node}, Farness: {farness}\")\n",
    "    \n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "    \n",
    "    print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "    print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # Maintain Top's length to top_k\n",
    "            print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        else:\n",
    "            print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "    \n",
    "        print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "        print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "        \n",
    "        if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "            print(Farn[Q[0][1]])\n",
    "            print(Farn[Top[-1]])\n",
    "            print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFinal Top 10 nodes by refined lower centrality bound:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        print(f\"{i}. Node: {node}, Refined Lower Centrality: {Farn[node]:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"com-youtube.ungraph.tsv\"  # Replace with your file path\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc22c00d-e4cb-47c3-bbe0-3726053d3bb1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37b4c14-7979-454a-8dc9-cc259de71350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_closeness_centrality(graph, top_k=10):\n",
    "    \"\"\"\n",
    "    Calculates the closeness centrality of nodes in a graph and returns the top k nodes with the highest centrality.\n",
    "\n",
    "    Args:\n",
    "        graph (networkx.Graph): The input graph.\n",
    "        top_k (int): The number of top nodes to return.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the top k nodes and their closeness centrality values.\n",
    "    \"\"\"\n",
    "    # Calculate closeness centrality for each node\n",
    "    closeness_centrality = nx.closeness_centrality(graph)\n",
    "\n",
    "    # Get the top k nodes with the highest closeness centrality\n",
    "    top_nodes = heapq.nlargest(top_k, closeness_centrality, key=closeness_centrality.get)\n",
    "    top_centrality = {node: closeness_centrality[node] for node in top_nodes}\n",
    "\n",
    "    return top_centrality\n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "\n",
    "    print(\"Calculating top k nodes by closeness centrality...\")\n",
    "    top_closeness = calculate_closeness_centrality(graph, top_k)\n",
    "\n",
    "    print(\"\\nTop 10 nodes by closeness centrality:\")\n",
    "    for i, (node, centrality) in enumerate(top_closeness.items(), 1):\n",
    "        print(f\"{i}. Node: {node}, Closeness Centrality: {centrality:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"com-youtube.ungraph.tsv\"  # Replace with your file path\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc32c1b-3e08-4688-87dd-400250b18b3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbde2816-d13a-4f3a-80d9-a7b4e710fbfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def pagerank_lower_bound(graph):\n",
    "    # Calculate PageRank for each node\n",
    "    page_ranks = nx.pagerank(graph)\n",
    "    \n",
    "    # Use the reciprocal of PageRank as a lower bound\n",
    "    lower_bounds = {node: 1 / page_ranks[node] for node in graph.nodes()}\n",
    "    \n",
    "    return lower_bounds\n",
    "\n",
    "def updateBoundsBFSCut(v, graph, x):\n",
    "    \"\"\"\n",
    "    Calculates a lower bound on the size of the cut separating vertex v.\n",
    "\n",
    "    Args:\n",
    "        v: The starting vertex.\n",
    "        graph: A networkx DiGraph. Nodes must have an 'r' attribute.\n",
    "        x: A threshold value.\n",
    "\n",
    "    Returns:\n",
    "        +âˆž if the cut value exceeds x; otherwise, returns the calculated cut value, \n",
    "        or the current Farn[v] if no change.\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    Q = [(0,v)] # Priority queue for BFS (distance, node)\n",
    "    heapq.heapify(Q)\n",
    "    visited = {v}\n",
    "    d = 0\n",
    "    S = 0\n",
    "    y = graph.degree(v) - 1\n",
    "    nd = 1\n",
    "\n",
    "    while Q:\n",
    "        dist, u = heapq.heappop(Q)\n",
    "        if dist > d:\n",
    "            d += 1\n",
    "            #LCUT calculation\n",
    "            LCUT = ((d+2)*(n-nd) + S - y )/(n-1)\n",
    "\n",
    "            if LCUT >= x:\n",
    "                return float('inf')\n",
    "            y = 0  # Reset y\n",
    "\n",
    "        for w in graph.neighbors(u):\n",
    "            if w not in visited:\n",
    "                visited.add(w)\n",
    "                dist_vw = nx.shortest_path_length(graph,v,w) # Assumes shortest path exists\n",
    "                heapq.heappush(Q,(dist_vw,w))\n",
    "                S += dist_vw  # Distance d(v,w)\n",
    "                y += graph.degree(w)\n",
    "                nd += 1\n",
    "            else:\n",
    "                LCUT = LCUT + 1/(n-1)\n",
    "\n",
    "    #Final calculation\n",
    "    LCUT_final = S / (n-1)\n",
    "\n",
    "    return LCUT_final\n",
    "\n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds using PageRank...\")\n",
    "\n",
    "    initial_lower_bounds = pagerank_lower_bound(graph)\n",
    "    \n",
    "    k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    print(f\"\\n{top_k} nodes with lowest initial farness values:\")\n",
    "    for node, farness in k_lowest_farness:\n",
    "        print(f\"Node: {node}, Farness: {farness}\")\n",
    "    \n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "    \n",
    "    print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "    print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        \n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # Keep Top length at top_k\n",
    "            print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        else:\n",
    "            print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "    \n",
    "        print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "        print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "        \n",
    "        if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "            print(Farn[v])\n",
    "            print(Farn[Top[-1]])\n",
    "            print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFinal Top 10 nodes by refined lower centrality bound:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        print(f\"{i}. Node: {node}, Refined Lower Centrality: {Farn[node]:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"com-youtube.ungraph.tsv\"  # Replace with your file path\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d28c72-091b-40e1-a0e1-692b2c17bda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def neighborhood_lower_bound(graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    lower_bounds = {}\n",
    "\n",
    "    k = 2\n",
    "    max_iterations = 100000  # Set maximum iterations\n",
    "\n",
    "    Y = {}\n",
    "    S_un = {}\n",
    "    nVisited = {}\n",
    "    finished = {}\n",
    "\n",
    "    for s in nodes:\n",
    "        degree_s = graph.degree(s)\n",
    "        Y[(1, s)] = degree_s\n",
    "        S_un[s] = degree_s\n",
    "        nVisited[s] = degree_s + 1\n",
    "        finished[s] = False\n",
    "\n",
    "    nFinished = 0\n",
    "\n",
    "    while nFinished < n and k <= max_iterations:\n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "            if k == 2:\n",
    "                Y[(k, s)] = sum(Y.get((k - 1, w), 0) for w in graph.neighbors(s)) - graph.degree(s)\n",
    "            else:\n",
    "                Y[(k, s)] = sum(Y.get((k - 1, w), 0) for w in graph.neighbors(s)) - Y.get((k - 2, s), 0) * (\n",
    "                            graph.degree(s) - 1)\n",
    "\n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "            y_k_minus_2 = Y.get((k - 2, s), 0)\n",
    "            y_k_minus_1 = Y.get((k - 1, s), 0)\n",
    "            nVisited[s] += y_k_minus_1\n",
    "\n",
    "            if nVisited[s] < n:\n",
    "                S_un[s] += k * y_k_minus_1\n",
    "            else:\n",
    "                S_un[s] += k * (n - (nVisited[s] - y_k_minus_1))\n",
    "                nFinished += 1\n",
    "                finished[s] = True\n",
    "            Y[(k - 2, s)] = y_k_minus_1\n",
    "            Y[(k - 1, s)] = Y[(k, s)]\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    for v in nodes:\n",
    "        lower_bounds[v] = S_un[v] / (n - 1)\n",
    "\n",
    "    return lower_bounds\n",
    "\n",
    "\n",
    "def updateBoundsBFSCut(v, graph, x):\n",
    "    \"\"\"\n",
    "    Calculate the lower bound of the cut size that separates vertex v.\n",
    "\n",
    "    Parameters:\n",
    "        v: Starting vertex.\n",
    "        graph: networkx DiGraph. Nodes must have 'r' attribute.\n",
    "        Farn: Dictionary that stores the lower bounds (must be initialized).\n",
    "        Top: List of top k nodes (must be initialized).\n",
    "        x: A threshold.\n",
    "\n",
    "    Returns:\n",
    "        If the cut value exceeds x, return +âˆž; otherwise, return the calculated cut value or the current Farn[v] if unchanged.\n",
    "    \"\"\"\n",
    "\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    Q = [(0, v)]  # Priority queue to track BFS (distance, node)\n",
    "    heapq.heapify(Q)\n",
    "    visited = {v}\n",
    "    d = 0\n",
    "    S = 0\n",
    "    y = graph.degree(v) - 1\n",
    "    nd = 1\n",
    "\n",
    "    while Q:\n",
    "        dist, u = heapq.heappop(Q)\n",
    "        if dist > d:\n",
    "            d += 1\n",
    "            # LCUT calculation\n",
    "            LCUT = ((d + 2) * (n - nd) + S - y) / (n - 1)\n",
    "\n",
    "            if LCUT >= x:\n",
    "                return float('inf')\n",
    "            y = 0  # Reset y\n",
    "\n",
    "        for w in graph.neighbors(u):\n",
    "            if w not in visited:\n",
    "                visited.add(w)\n",
    "                dist_vw = nx.shortest_path_length(graph, v, w)  # Assume a shortest path exists\n",
    "                heapq.heappush(Q, (dist_vw, w))\n",
    "                S += dist_vw  # Distance d(v, w)\n",
    "                y += graph.degree(w)\n",
    "                nd += 1\n",
    "            else:\n",
    "                LCUT += 1 / (n - 1)\n",
    "\n",
    "    # Final calculation\n",
    "    LCUT_final = S / (n - 1)\n",
    "\n",
    "    return LCUT_final\n",
    "\n",
    "\n",
    "def pagerank_lower_bound(graph):\n",
    "    # Calculate PageRank for each node\n",
    "    page_ranks = nx.pagerank(graph)\n",
    "\n",
    "    # Use the reciprocal of PageRank as a lower bound\n",
    "    lower_bounds = {node: 1 / page_ranks[node] for node in graph.nodes()}\n",
    "\n",
    "    return lower_bounds\n",
    "\n",
    "\n",
    "def calculate_top_k_closeness_centrality_nx(graph, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(\"Calculating closeness centrality using NetworkX...\")\n",
    "    closeness_centrality = nx.closeness_centrality(graph)\n",
    "\n",
    "    top_k_nodes = heapq.nlargest(top_k, closeness_centrality.items(), key=lambda x: x[1])\n",
    "\n",
    "    print(f\"\\nTop {top_k} nodes by closeness centrality (NetworkX):\")\n",
    "    for i, (node, centrality) in enumerate(top_k_nodes, 1):\n",
    "        print(f\"{i}. Node: {node}, Closeness Centrality: {centrality:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken for NetworkX calculation: {total_time:.4f} seconds\")\n",
    "\n",
    "    return [node for node, _ in top_k_nodes]\n",
    "\n",
    "\n",
    "def calculate_top_k_closeness_centrality_paper(graph, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Processing graph with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds...\")\n",
    "\n",
    "    initial_lower_bounds = neighborhood_lower_bound(graph)\n",
    "    k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    # print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    # for node, farness in k_lowest_farness:\n",
    "    # print(f\"Node: {node}, Farness: {farness}\")\n",
    "\n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "\n",
    "    # print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "\n",
    "    # print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            # print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # Maintain Top's length to top_k\n",
    "            # print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        # else:\n",
    "        # print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "\n",
    "        # print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "\n",
    "        # print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "\n",
    "        # if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "        # print(Farn[Q[0][1]])\n",
    "        # print(Farn[Top[-1]])\n",
    "        # print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "        # break\n",
    "\n",
    "    # print(\"\\nFinal Top 10 nodes by refined closeness centrality:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        closeness = 1 / Farn[node] if Farn[node] != 0 else float('inf')\n",
    "        print(f\"{i}. Node: {node}, Refined Closeness Centrality paper: {closeness:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return Top\n",
    "\n",
    "\n",
    "def calculate_top_k_closeness_centrality_pagerank(graph, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Processing graph with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds...\")\n",
    "\n",
    "    initial_lower_bounds = pagerank_lower_bound(graph)\n",
    "    k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    # print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    # for node, farness in k_lowest_farness:\n",
    "    # print(f\"Node: {node}, Farness: {farness}\")\n",
    "\n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "\n",
    "    # print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "\n",
    "    # print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            # print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # Maintain Top's length to top_k\n",
    "            # print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        # else:\n",
    "        # print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "\n",
    "        # print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "\n",
    "        # print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "\n",
    "        # if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "        # print(Farn[Q[0][1]])\n",
    "        # print(Farn[Top[-1]])\n",
    "        # print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "        # break\n",
    "\n",
    "    print(\"\\nFinal Top 10 nodes by refined closeness centrality:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        closeness = 1 / Farn[node] if Farn[node] != 0 else float('inf')\n",
    "        print(f\"{i}. Node: {node}, Refined Closeness Centrality pagerank: {closeness:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return Top\n",
    "\n",
    "\n",
    "def evaluate_methods(graph):\n",
    "\n",
    "    graph = graph\n",
    "\n",
    "    start_time = time.time()\n",
    "    nx_top_k_nodes = calculate_top_k_closeness_centrality_nx(graph, top_k=top_k)\n",
    "    time_nx_trials.append(time.time() - start_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    paper_top_k_nodes = calculate_top_k_closeness_centrality_paper(graph, top_k=top_k)\n",
    "    time_paper_trials.append(time.time() - start_time)\n",
    "\n",
    "    start_time = time.time()\n",
    "    pagerank_top_k_nodes = calculate_top_k_closeness_centrality_pagerank(graph, top_k=top_k)\n",
    "    time_pagerank_trials.append(time.time() - start_time)\n",
    "\n",
    "    acc_paper_trials.append(len(set(paper_top_k_nodes) & set(nx_top_k_nodes)) / top_k)\n",
    "    acc_pagerank_trials.append(len(set(pagerank_top_k_nodes) & set(nx_top_k_nodes)) / top_k)\n",
    "\n",
    "    common_nodes_nx_paper = set(paper_top_k_nodes) & set(nx_top_k_nodes)\n",
    "    print(f\"\\nNumber of common nodes in paper and nx methods: {len(common_nodes_nx_paper)}\")\n",
    "    print(f\"Common nodes: {common_nodes_nx_paper}\")\n",
    "\n",
    "    common_nodes_nx_pagerank = set(pagerank_top_k_nodes) & set(nx_top_k_nodes)\n",
    "    print(f\"\\nNumber of common nodes in pagerank and nx methods: {len(common_nodes_nx_pagerank)}\")\n",
    "    print(f\"Common nodes: {common_nodes_nx_pagerank}\")\n",
    "\n",
    "    print(f\"Completed evaluation for graph with {num_nodes} nodes and {num_edges} edges\")\n",
    "\n",
    "    return accuracy_paper, accuracy_pagerank, time_nx, time_paper, time_pagerank\n",
    "\n",
    "\n",
    "def plot_results(num_nodes_list, accuracy_paper, accuracy_pagerank, time_nx, time_paper, time_pagerank):\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(num_nodes_list, accuracy_paper, label='Paper Method')\n",
    "    plt.plot(num_nodes_list, accuracy_pagerank, label='PageRank Method')\n",
    "    plt.xlabel('Number of Nodes')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy Comparison')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(num_nodes_list, time_nx, label='NetworkX')\n",
    "    plt.plot(num_nodes_list, time_paper, label='Paper Method')\n",
    "    plt.plot(num_nodes_list, time_pagerank, label='PageRank Method')\n",
    "    plt.xlabel('Number of Nodes')\n",
    "    plt.ylabel('Time (seconds)')\n",
    "    plt.title('Time Comparison')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def load_graph_from_tsv(file_path):\n",
    "    df = pd.read_csv(file_path, sep='\\t', header=None, names=['source', 'target'])\n",
    "\n",
    "    graph = nx.from_pandas_edgelist(df, 'source', 'target')\n",
    "\n",
    "    print(f\"Graph loaded with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    return graph\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = 'com-youtube.ungraph.tsv'\n",
    "    graph = load_graph_from_tsv(file_path)\n",
    "\n",
    "    accuracy_paper, accuracy_pagerank, time_nx, time_paper, time_pagerank = evaluate_methods(num_nodes_list\n",
    "    print(f'\\nNumber of common nodes in pagerank and nx methods: {len(common_nodes_nx_pagerank)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
