{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2536e023-25f4-4db7-b2a5-79f29db6f9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bcae778c-70b7-47c2-9413-95d0c3dde690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating random graph...\n",
      "Random undirected graph created with 100 nodes and 500 edges.\n",
      "\n",
      "Processing graph for centrality using paper method...\n",
      "Processing graph with 100 nodes and 500 edges.\n",
      "Calculating initial lower bounds...\n",
      "\n",
      "10 nodes with lowest farness values:\n",
      "Node: 6, Farness: 2.484848484848485\n",
      "Node: 77, Farness: 2.484848484848485\n",
      "Node: 93, Farness: 2.484848484848485\n",
      "Node: 10, Farness: 2.515151515151515\n",
      "Node: 72, Farness: 2.515151515151515\n",
      "Node: 23, Farness: 2.5454545454545454\n",
      "Node: 25, Farness: 2.5454545454545454\n",
      "Node: 41, Farness: 2.5454545454545454\n",
      "Node: 65, Farness: 2.5454545454545454\n",
      "Node: 74, Farness: 2.5454545454545454\n",
      "Processing node 6\n",
      "Refined bound for node 6: 1.9898989898989898\n",
      "Added node 6 to Top. Current Top: [6]\n",
      "Processing node 77\n",
      "Refined bound for node 77: 1.9696969696969697\n",
      "Added node 77 to Top. Current Top: [77, 6]\n",
      "Processing node 93\n",
      "Refined bound for node 93: 2.0202020202020203\n",
      "Added node 93 to Top. Current Top: [77, 6, 93]\n",
      "Processing node 10\n",
      "Refined bound for node 10: 2.0202020202020203\n",
      "Added node 10 to Top. Current Top: [77, 6, 93, 10]\n",
      "Processing node 72\n",
      "Refined bound for node 72: 2.0303030303030303\n",
      "Added node 72 to Top. Current Top: [77, 6, 93, 10, 72]\n",
      "Processing node 23\n",
      "Refined bound for node 23: 2.0202020202020203\n",
      "Added node 23 to Top. Current Top: [77, 6, 93, 10, 23, 72]\n",
      "Processing node 25\n",
      "Refined bound for node 25: 2.04040404040404\n",
      "Added node 25 to Top. Current Top: [77, 6, 93, 10, 23, 72, 25]\n",
      "Processing node 41\n",
      "Refined bound for node 41: 2.090909090909091\n",
      "Added node 41 to Top. Current Top: [77, 6, 93, 10, 23, 72, 25, 41]\n",
      "Processing node 65\n",
      "Refined bound for node 65: 2.0707070707070705\n",
      "Added node 65 to Top. Current Top: [77, 6, 93, 10, 23, 72, 25, 65, 41]\n",
      "Processing node 74\n",
      "Refined bound for node 74: 2.0\n",
      "Added node 74 to Top. Current Top: [77, 6, 74, 93, 10, 23, 72, 25, 65, 41]\n",
      "Processing node 80\n",
      "Refined bound for node 80: 1.9898989898989898\n",
      "Added node 80 to Top and trimmed. Current Top: [77, 6, 80, 74, 93, 10, 23, 72, 25, 65]\n",
      "Processing node 3\n",
      "Refined bound for node 3: 2.0505050505050506\n",
      "Added node 3 to Top and trimmed. Current Top: [77, 6, 80, 74, 93, 10, 23, 72, 25, 3]\n",
      "Processing node 71\n",
      "Refined bound for node 71: 2.111111111111111\n",
      "2.5757575757575757\n",
      "2.0505050505050506\n",
      "Remaining nodes in Q cannot improve Top. Stopping.\n",
      "\n",
      "Final Top 10 nodes by refined closeness centrality:\n",
      "1. Node: 77, Refined Closeness Centrality: 0.5076923077\n",
      "2. Node: 6, Refined Closeness Centrality: 0.5025380711\n",
      "3. Node: 80, Refined Closeness Centrality: 0.5025380711\n",
      "4. Node: 74, Refined Closeness Centrality: 0.5000000000\n",
      "5. Node: 93, Refined Closeness Centrality: 0.4950000000\n",
      "6. Node: 10, Refined Closeness Centrality: 0.4950000000\n",
      "7. Node: 23, Refined Closeness Centrality: 0.4950000000\n",
      "8. Node: 72, Refined Closeness Centrality: 0.4925373134\n",
      "9. Node: 25, Refined Closeness Centrality: 0.4900990099\n",
      "10. Node: 3, Refined Closeness Centrality: 0.4876847291\n",
      "\n",
      "Total time taken: 0.0066 seconds\n",
      "\n",
      "Top k nodes returned by our method:\n",
      "[77, 6, 80, 74, 93, 10, 23, 72, 25, 3]\n",
      "\n",
      "Processing graph for centrality using pagerank method...\n",
      "Processing graph with 100 nodes and 500 edges.\n",
      "Calculating initial lower bounds...\n",
      "\n",
      "10 nodes with lowest farness values:\n",
      "Node: 93, Farness: 62.76096746297224\n",
      "Node: 6, Farness: 63.42990988681942\n",
      "Node: 77, Farness: 63.53255594249601\n",
      "Node: 72, Farness: 65.75733686603057\n",
      "Node: 10, Farness: 66.77000443436667\n",
      "Node: 25, Farness: 68.71845638063769\n",
      "Node: 74, Farness: 69.98552927681285\n",
      "Node: 41, Farness: 70.49309384093756\n",
      "Node: 65, Farness: 70.64837283449263\n",
      "Node: 23, Farness: 71.05442213879637\n",
      "Processing node 93\n",
      "Refined bound for node 93: 2.0202020202020203\n",
      "Added node 93 to Top. Current Top: [93]\n",
      "Processing node 6\n",
      "Refined bound for node 6: 1.9898989898989898\n",
      "Added node 6 to Top. Current Top: [6, 93]\n",
      "Processing node 77\n",
      "Refined bound for node 77: 1.9696969696969697\n",
      "Added node 77 to Top. Current Top: [77, 6, 93]\n",
      "Processing node 72\n",
      "Refined bound for node 72: 2.0303030303030303\n",
      "Added node 72 to Top. Current Top: [77, 6, 93, 72]\n",
      "Processing node 10\n",
      "Refined bound for node 10: 2.0202020202020203\n",
      "Added node 10 to Top. Current Top: [77, 6, 93, 10, 72]\n",
      "Processing node 25\n",
      "Refined bound for node 25: 2.04040404040404\n",
      "Added node 25 to Top. Current Top: [77, 6, 93, 10, 72, 25]\n",
      "Processing node 74\n",
      "Refined bound for node 74: 2.0\n",
      "Added node 74 to Top. Current Top: [77, 6, 74, 93, 10, 72, 25]\n",
      "Processing node 41\n",
      "Refined bound for node 41: 2.090909090909091\n",
      "Added node 41 to Top. Current Top: [77, 6, 74, 93, 10, 72, 25, 41]\n",
      "Processing node 65\n",
      "Refined bound for node 65: 2.0707070707070705\n",
      "Added node 65 to Top. Current Top: [77, 6, 74, 93, 10, 72, 25, 65, 41]\n",
      "Processing node 23\n",
      "Refined bound for node 23: 2.0202020202020203\n",
      "Added node 23 to Top. Current Top: [77, 6, 74, 93, 10, 23, 72, 25, 65, 41]\n",
      "Processing node 80\n",
      "Refined bound for node 80: 1.9898989898989898\n",
      "Added node 80 to Top and trimmed. Current Top: [77, 6, 80, 74, 93, 10, 23, 72, 25, 65]\n",
      "Processing node 3\n",
      "Refined bound for node 3: 2.0505050505050506\n",
      "Added node 3 to Top and trimmed. Current Top: [77, 6, 80, 74, 93, 10, 23, 72, 25, 3]\n",
      "Processing node 71\n",
      "Refined bound for node 71: 2.111111111111111\n",
      "75.72521330258338\n",
      "2.0505050505050506\n",
      "Remaining nodes in Q cannot improve Top. Stopping.\n",
      "\n",
      "Final Top 10 nodes by refined closeness centrality:\n",
      "1. Node: 77, Refined Closeness Centrality: 0.5076923077\n",
      "2. Node: 6, Refined Closeness Centrality: 0.5025380711\n",
      "3. Node: 80, Refined Closeness Centrality: 0.5025380711\n",
      "4. Node: 74, Refined Closeness Centrality: 0.5000000000\n",
      "5. Node: 93, Refined Closeness Centrality: 0.4950000000\n",
      "6. Node: 10, Refined Closeness Centrality: 0.4950000000\n",
      "7. Node: 23, Refined Closeness Centrality: 0.4950000000\n",
      "8. Node: 72, Refined Closeness Centrality: 0.4925373134\n",
      "9. Node: 25, Refined Closeness Centrality: 0.4900990099\n",
      "10. Node: 3, Refined Closeness Centrality: 0.4876847291\n",
      "\n",
      "Total time taken: 0.0086 seconds\n",
      "\n",
      "Top k nodes returned by our method:\n",
      "[77, 6, 80, 74, 93, 10, 23, 72, 25, 3]\n",
      "\n",
      "Calculating top k nodes using NetworkX...\n",
      "Calculating closeness centrality using NetworkX...\n",
      "\n",
      "Top 10 nodes by closeness centrality (NetworkX):\n",
      "1. Node: 77, Closeness Centrality: 0.5076923077\n",
      "2. Node: 6, Closeness Centrality: 0.5025380711\n",
      "3. Node: 80, Closeness Centrality: 0.5025380711\n",
      "4. Node: 74, Closeness Centrality: 0.5000000000\n",
      "5. Node: 10, Closeness Centrality: 0.4950000000\n",
      "6. Node: 23, Closeness Centrality: 0.4950000000\n",
      "7. Node: 93, Closeness Centrality: 0.4950000000\n",
      "8. Node: 72, Closeness Centrality: 0.4925373134\n",
      "9. Node: 25, Closeness Centrality: 0.4900990099\n",
      "10. Node: 3, Closeness Centrality: 0.4876847291\n",
      "\n",
      "Total time taken for NetworkX calculation: 0.0027 seconds\n",
      "\n",
      "Top k nodes returned by NetworkX:\n",
      "[77, 6, 80, 74, 10, 23, 93, 72, 25, 3]\n",
      "\n",
      "Number of common nodes in paper and nx methods: 10\n",
      "Common nodes: {3, 6, 72, 10, 74, 77, 80, 23, 25, 93}\n",
      "\n",
      "Number of common nodes in pagerank and nx methods: 10\n",
      "Common nodes: {3, 6, 72, 10, 74, 77, 80, 23, 25, 93}\n"
     ]
    }
   ],
   "source": [
    "def neighborhood_lower_bound(graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    lower_bounds = {}\n",
    "\n",
    "    k = 2\n",
    "    max_iterations = 100000  # Set maximum iterations\n",
    "\n",
    "    Y = {}\n",
    "    S_un = {}\n",
    "    nVisited = {}\n",
    "    finished = {}\n",
    "\n",
    "    for s in nodes:\n",
    "        degree_s = graph.degree(s)\n",
    "        Y[(1, s)] = degree_s\n",
    "        S_un[s] = degree_s\n",
    "        nVisited[s] = degree_s + 1\n",
    "        finished[s] = False\n",
    "\n",
    "    nFinished = 0\n",
    "\n",
    "    while nFinished < n and k <= max_iterations:\n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "            if k == 2:\n",
    "                Y[(k, s)] = sum(Y.get((k-1, w), 0) for w in graph.neighbors(s)) - graph.degree(s)\n",
    "            else:\n",
    "                Y[(k, s)] = sum(Y.get((k-1, w), 0) for w in graph.neighbors(s)) - Y.get((k-2, s), 0) * (graph.degree(s) - 1)\n",
    "        \n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "            y_k_minus_2 = Y.get((k-2, s), 0)\n",
    "            y_k_minus_1 = Y.get((k-1, s), 0)\n",
    "            nVisited[s] += y_k_minus_1\n",
    "            \n",
    "            if nVisited[s] < n:\n",
    "                S_un[s] += k * y_k_minus_1\n",
    "            else:\n",
    "                S_un[s] += k *(n - (nVisited[s] - y_k_minus_1))\n",
    "                nFinished += 1\n",
    "                finished[s] = True\n",
    "            Y[(k-2, s)] = y_k_minus_1\n",
    "            Y[(k-1, s)] = Y[(k, s)]\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    for v in nodes:\n",
    "        lower_bounds[v] = S_un[v] / (n - 1)\n",
    "\n",
    "    return lower_bounds\n",
    "\n",
    "def updateBoundsBFSCut(v, graph, x):\n",
    "    \"\"\"\n",
    "    Calculate the lower bound of the cut size that separates vertex v.\n",
    "\n",
    "    Parameters:\n",
    "        v: Starting vertex.\n",
    "        graph: networkx DiGraph. Nodes must have 'r' attribute.\n",
    "        Farn: Dictionary that stores the lower bounds (must be initialized).\n",
    "        Top: List of top k nodes (must be initialized).\n",
    "        x: A threshold.\n",
    "\n",
    "    Returns:\n",
    "        If the cut value exceeds x, return +∞; otherwise, return the calculated cut value or the current Farn[v] if unchanged.\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    Q = [(0, v)]  # Priority queue to track BFS (distance, node)\n",
    "    heapq.heapify(Q)\n",
    "    visited = {v}\n",
    "    d = 0\n",
    "    S = 0\n",
    "    y = graph.degree(v) - 1\n",
    "    nd = 1\n",
    "\n",
    "    while Q:\n",
    "        dist, u = heapq.heappop(Q)\n",
    "        if dist > d:\n",
    "            d += 1\n",
    "            # LCUT calculation\n",
    "            LCUT = ((d + 2) * (n - nd) + S - y) / (n - 1)\n",
    "\n",
    "            if LCUT >= x:\n",
    "                return float('inf')\n",
    "            y = 0  # Reset y\n",
    "\n",
    "        for w in graph.neighbors(u):\n",
    "            if w not in visited:\n",
    "                visited.add(w)\n",
    "                dist_vw = nx.shortest_path_length(graph, v, w)  # Assume a shortest path exists\n",
    "                heapq.heappush(Q, (dist_vw, w))\n",
    "                S += dist_vw  # Distance d(v, w)\n",
    "                y += graph.degree(w)\n",
    "                nd += 1\n",
    "            else:\n",
    "                LCUT += 1 / (n - 1)\n",
    "\n",
    "    # Final calculation\n",
    "    LCUT_final = S / (n - 1)\n",
    "\n",
    "    return LCUT_final\n",
    "\n",
    "def pagerank_lower_bound(graph):\n",
    "    # Calculate PageRank for each node\n",
    "    page_ranks = nx.pagerank(graph)\n",
    "    \n",
    "    # Use the reciprocal of PageRank as a lower bound\n",
    "    lower_bounds = {node: 1 / page_ranks[node] for node in graph.nodes()}\n",
    "    \n",
    "    return lower_bounds\n",
    "\n",
    "def calculate_top_k_closeness_centrality_nx(graph, top_k=10):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(\"Calculating closeness centrality using NetworkX...\")\n",
    "    closeness_centrality = nx.closeness_centrality(graph)\n",
    "    \n",
    "    top_k_nodes = heapq.nlargest(top_k, closeness_centrality.items(), key=lambda x: x[1])\n",
    "    \n",
    "    print(f\"\\nTop {top_k} nodes by closeness centrality (NetworkX):\")\n",
    "    for i, (node, centrality) in enumerate(top_k_nodes, 1):\n",
    "        print(f\"{i}. Node: {node}, Closeness Centrality: {centrality:.10f}\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken for NetworkX calculation: {total_time:.4f} seconds\")\n",
    "    \n",
    "    return [node for node, _ in top_k_nodes]\n",
    "\n",
    "def create_random_undirected_graph(num_nodes=1000, num_edges=5000):\n",
    "    graph = nx.gnm_random_graph(num_nodes, num_edges)\n",
    "    print(f\"Random undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    return graph\n",
    "\n",
    "def calculate_top_k_closeness_centrality_paper(graph, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Processing graph with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds...\")\n",
    "\n",
    "    initial_lower_bounds = neighborhood_lower_bound(graph)\n",
    "    k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    for node, farness in k_lowest_farness:\n",
    "        print(f\"Node: {node}, Farness: {farness}\")\n",
    "    \n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "    \n",
    "    print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "    print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # Maintain Top's length to top_k\n",
    "            print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        else:\n",
    "            print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "    \n",
    "        print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "        print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "        \n",
    "        if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "            print(Farn[Q[0][1]])\n",
    "            print(Farn[Top[-1]])\n",
    "            print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFinal Top 10 nodes by refined closeness centrality:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        closeness = 1 / Farn[node] if Farn[node] != 0 else float('inf')\n",
    "        print(f\"{i}. Node: {node}, Refined Closeness Centrality: {closeness:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return Top\n",
    "\n",
    "def calculate_top_k_closeness_centrality_pagerank(graph, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    print(f\"Processing graph with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds...\")\n",
    "\n",
    "    initial_lower_bounds = pagerank_lower_bound(graph)\n",
    "    k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    for node, farness in k_lowest_farness:\n",
    "        print(f\"Node: {node}, Farness: {farness}\")\n",
    "    \n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "    \n",
    "    print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "    print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # Maintain Top's length to top_k\n",
    "            print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        else:\n",
    "            print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "    \n",
    "        print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "        print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "        \n",
    "        if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "            print(Farn[Q[0][1]])\n",
    "            print(Farn[Top[-1]])\n",
    "            print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFinal Top 10 nodes by refined closeness centrality:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        closeness = 1 / Farn[node] if Farn[node] != 0 else float('inf')\n",
    "        print(f\"{i}. Node: {node}, Refined Closeness Centrality: {closeness:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return Top\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Creating random graph...\")\n",
    "    random_graph = create_random_undirected_graph(num_nodes=100, num_edges=500)\n",
    "    \n",
    "    print(\"\\nProcessing graph for centrality using paper method...\")\n",
    "    paper_top_k_nodes = calculate_top_k_closeness_centrality_paper(random_graph, top_k=10)\n",
    "    \n",
    "    print(\"\\nTop k nodes returned by our method:\")\n",
    "    print(paper_top_k_nodes)\n",
    "\n",
    "    print(\"\\nProcessing graph for centrality using pagerank method...\")\n",
    "    pagerank_top_k_nodes = calculate_top_k_closeness_centrality_pagerank(random_graph, top_k=10)\n",
    "    \n",
    "    print(\"\\nTop k nodes returned by our method:\")\n",
    "    print(pagerank_top_k_nodes)\n",
    "    \n",
    "    print(\"\\nCalculating top k nodes using NetworkX...\")\n",
    "    nx_top_k_nodes = calculate_top_k_closeness_centrality_nx(random_graph, top_k=10)\n",
    "    \n",
    "    print(\"\\nTop k nodes returned by NetworkX:\")\n",
    "    print(nx_top_k_nodes)\n",
    "    \n",
    "    # 比较两种方法的结果\n",
    "    common_nodes_nx_paper = set(paper_top_k_nodes) & set(nx_top_k_nodes)\n",
    "    print(f\"\\nNumber of common nodes in paper and nx methods: {len(common_nodes_nx_paper)}\")\n",
    "    print(f\"Common nodes: {common_nodes_nx_paper}\")\n",
    "    \n",
    "    common_nodes_nx_pagerank = set(pagerank_top_k_nodes) & set(nx_top_k_nodes)\n",
    "    print(f\"\\nNumber of common nodes in pagerank and nx methods: {len(common_nodes_nx_pagerank)}\")\n",
    "    print(f\"Common nodes: {common_nodes_nx_pagerank}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283ddf7-8469-401b-a31b-9c92608205c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ccee47-81f3-462c-b774-9e4f1d70d27e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c17afa0-8eeb-4702-b305-badd4feed28a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab04cf54-6bf7-4f09-a68b-ea6261a4d4a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
