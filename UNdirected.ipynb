{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "62c6706a-cf4c-4246-931c-84b0c8a68b29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Undirected graph created with 2158 nodes and 5136 edges.\n",
      "Calculating initial lower bounds...\n",
      "\n",
      "Top 10 nodes by initial lower centrality bound:\n",
      "1. Node: 1849, Initial Lower Centrality: 255.5108947612\n",
      "2. Node: 1811, Initial Lower Centrality: 214.2749188688\n",
      "3. Node: 1813, Initial Lower Centrality: 214.2749188688\n",
      "4. Node: 1814, Initial Lower Centrality: 214.2749188688\n",
      "5. Node: 1815, Initial Lower Centrality: 214.2749188688\n",
      "6. Node: 1816, Initial Lower Centrality: 214.2749188688\n",
      "7. Node: 1817, Initial Lower Centrality: 214.2749188688\n",
      "8. Node: 1820, Initial Lower Centrality: 214.2749188688\n",
      "9. Node: 1821, Initial Lower Centrality: 214.2749188688\n",
      "10. Node: 1822, Initial Lower Centrality: 214.2749188688\n",
      "\n",
      "Refining bounds for top 10 nodes...\n",
      "\n",
      "Total time taken: 0.0789 seconds\n",
      "\n",
      "Top 10 nodes by refined lower centrality bound:\n",
      "1. Node: 689, Refined Lower Centrality: 2036.9439035698\n",
      "2. Node: 526, Refined Lower Centrality: 728.9995363931\n",
      "3. Node: 562, Refined Lower Centrality: 728.9995363931\n",
      "4. Node: 567, Refined Lower Centrality: 728.9995363931\n",
      "5. Node: 581, Refined Lower Centrality: 728.9995363931\n",
      "6. Node: 594, Refined Lower Centrality: 728.9995363931\n",
      "7. Node: 597, Refined Lower Centrality: 728.9995363931\n",
      "8. Node: 602, Refined Lower Centrality: 728.9995363931\n",
      "9. Node: 606, Refined Lower Centrality: 728.9995363931\n",
      "10. Node: 634, Refined Lower Centrality: 728.9995363931\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def neighborhood_lower_bound(graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    lower_bounds = {}\n",
    "\n",
    "    k = 2\n",
    "    max_iterations = 1000  # 设置最大迭代次数\n",
    "\n",
    "    Y = {}\n",
    "    S_un = {}\n",
    "    nVisited = {}\n",
    "    finished = {}\n",
    "\n",
    "    for s in nodes:\n",
    "        degree_s = graph.degree(s)\n",
    "        Y[(k - 1, s)] = degree_s\n",
    "        S_un[(k - 1, s)] = degree_s\n",
    "        nVisited[s] = degree_s + 1\n",
    "        finished[s] = False\n",
    "\n",
    "    nFinished = 0\n",
    "    while nFinished < n and k <= max_iterations:\n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "\n",
    "            if k == 2:\n",
    "                Y[(k, s)] = sum(Y[(k - 1, w)] for w in graph.neighbors(s)) - graph.degree(s)\n",
    "            elif k > 2:\n",
    "                Y[(k, s)] = sum(Y[(k - 1, w)] for w in graph.neighbors(s)) - Y.get((k - 2, s), 0) * (graph.degree(s) - 1)\n",
    "            else:\n",
    "                Y[(k, s)] = 0\n",
    "\n",
    "            nVisited[s] += Y.get((k - 1, s), 0)\n",
    "\n",
    "            S_un[(k, s)] = S_un.get((k - 1, s), 0) + k * Y.get((k - 1, s), 0)\n",
    "\n",
    "            if nVisited[s] >= n:\n",
    "                finished[s] = True\n",
    "                nFinished += 1\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    for v in nodes:\n",
    "        lower_bounds[v] = S_un.get((k - 1, v), 0) / (n-1)\n",
    "\n",
    "    return lower_bounds\n",
    "\n",
    "\n",
    "def updateBoundsLB(graph, s):\n",
    "    \"\"\"\n",
    "    计算无向图中割集大小的下界。\n",
    "\n",
    "    参数：\n",
    "        s: 源节点。\n",
    "\n",
    "    返回：\n",
    "        一个字典，键是节点，值是它们的下界割集大小。\n",
    "    \"\"\"\n",
    "\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    d = nx.single_source_shortest_path_length(graph, s)\n",
    "    maxD = max(d.values())\n",
    "    sumL = {i:0 for i in range(maxD+2)}\n",
    "    sumG = {i:0 for i in range(maxD+2)}\n",
    "\n",
    "    Gamma = {i: [] for i in range(maxD + 1)}\n",
    "    for v, dist in d.items():\n",
    "        Gamma[dist].append(v)\n",
    "\n",
    "    for i in range(maxD+1):\n",
    "        sumL[i+1] = sumL[i] + len(Gamma[i]) #这是伪代码中的yi\n",
    "\n",
    "\n",
    "    sumG[maxD+1] = len(graph.nodes)\n",
    "    for i in range(maxD,0,-1):\n",
    "        sumG[i] = n - sumL[i+1]\n",
    "\n",
    "    L = {}\n",
    "    L[1] = sumL[1] + sumL[2] + sumG[2] -2\n",
    "\n",
    "\n",
    "    for i in range(2, maxD+1):\n",
    "        L[i] = L[i-1] + sumL[i-2] - sumG[i+1]\n",
    "\n",
    "    LB_s = {}\n",
    "    for i in range(1, maxD+1):\n",
    "        for v in Gamma[i]:\n",
    "            #LB 计算，注意 r(v) 应该定义为图中每个节点的属性\n",
    "            LB_s[v] = L[i] - graph.degree(v) / (len(graph.nodes)-1) \n",
    "\n",
    "\n",
    "    return LB_s\n",
    "    \n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds...\")\n",
    "    lower_bounds = neighborhood_lower_bound(graph)\n",
    "    nx.set_node_attributes(graph, lower_bounds, 'lower_centrality')\n",
    "\n",
    "    # 获取并打印 top k 个节点（基于初始下界）\n",
    "    sorted_centralities = sorted(\n",
    "        lower_bounds.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:top_k]\n",
    "\n",
    "    print(f\"\\nTop {top_k} nodes by initial lower centrality bound:\")\n",
    "    for i, (node, centrality) in enumerate(sorted_centralities, 1):\n",
    "        print(f\"{i}. Node: {node}, Initial Lower Centrality: {centrality:.10f}\")\n",
    "\n",
    "    print(f\"\\nRefining bounds for top {top_k} nodes...\")\n",
    "    for node, _ in sorted_centralities:\n",
    "        refined_bounds = updateBoundsLB(graph, node)\n",
    "        nx.set_node_attributes(graph, {v: {'refined_lower_centrality': b} for v, b in refined_bounds.items()})\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    # 获取并打印精炼后的top 10结果\n",
    "    refined_centralities = nx.get_node_attributes(graph, 'refined_lower_centrality')\n",
    "    sorted_refined_centralities = sorted(\n",
    "        refined_centralities.items(),\n",
    "        key=lambda x: x[1],\n",
    "        reverse=True\n",
    "    )[:10]\n",
    "\n",
    "    print(\"\\nTop 10 nodes by refined lower centrality bound:\")\n",
    "    for i, (node, centrality) in enumerate(sorted_refined_centralities, 1):\n",
    "        print(f\"{i}. Node: {node}, Refined Lower Centrality: {centrality:.10f}\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"medium.tsv\"\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9fb8fafd-e316-46b0-818c-60b3235881ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Undirected graph created with 2158 nodes and 5136 edges.\n",
      "\n",
      "20 nodes with lowest farness values:\n",
      "Node: 575, Farness: 3.001390820584145\n",
      "Node: 374, Farness: 3.008344923504868\n",
      "Node: 851, Farness: 3.0139082058414464\n",
      "Node: 222, Farness: 3.044506258692629\n",
      "Node: 46, Farness: 3.097357440890125\n",
      "Node: 588, Farness: 3.09874826147427\n",
      "Node: 682, Farness: 3.2962447844228095\n",
      "Node: 628, Farness: 3.305980528511822\n",
      "Node: 529, Farness: 3.3310152990264257\n",
      "Node: 245, Farness: 3.388038942976356\n",
      "Node: 817, Farness: 3.4089012517385258\n",
      "Node: 770, Farness: 3.4255910987482614\n",
      "Node: 118, Farness: 3.49095966620306\n",
      "Node: 116, Farness: 3.4937413073713492\n",
      "Node: 717, Farness: 3.5076495132127956\n",
      "Node: 51, Farness: 3.5424200278164117\n",
      "Node: 22, Farness: 3.556328233657858\n",
      "Node: 572, Farness: 3.603616133518776\n",
      "Node: 270, Farness: 3.631432545201669\n",
      "Node: 611, Farness: 3.6481223922114046\n",
      "\n",
      "No nodes with negative farness values found.\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "No nodes with negative farness values found (after update).\n",
      "\n",
      "Total time taken: 0.0910 seconds\n",
      "\n",
      "Top 20 nodes:\n",
      "1. Node: 575, Refined Lower Centrality: 3.0013908206\n",
      "2. Node: 588, Refined Lower Centrality: 0.9935095039\n",
      "3. Node: 245, Refined Lower Centrality: 0.9930458971\n",
      "4. Node: 529, Refined Lower Centrality: 0.9925822902\n",
      "5. Node: 270, Refined Lower Centrality: 0.9921186834\n",
      "6. Node: 628, Refined Lower Centrality: 0.9916550765\n",
      "7. Node: 770, Refined Lower Centrality: 0.9911914696\n",
      "8. Node: 572, Refined Lower Centrality: 0.9911914696\n",
      "9. Node: 374, Refined Lower Centrality: 0.9809921187\n",
      "10. Node: 118, Refined Lower Centrality: 0.9805285118\n",
      "11. Node: 51, Refined Lower Centrality: 0.9805285118\n",
      "12. Node: 116, Refined Lower Centrality: 0.9800649050\n",
      "13. Node: 611, Refined Lower Centrality: 0.9791376912\n",
      "14. Node: 22, Refined Lower Centrality: 0.9735744089\n",
      "15. Node: 222, Refined Lower Centrality: 0.9694019471\n",
      "16. Node: 851, Refined Lower Centrality: 0.9582753825\n",
      "17. Node: 717, Refined Lower Centrality: 0.9397311080\n",
      "18. Node: 682, Refined Lower Centrality: 0.9383402874\n",
      "19. Node: 817, Refined Lower Centrality: 0.9253592953\n",
      "20. Node: 46, Refined Lower Centrality: 0.9156235512\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def neighborhood_lower_bound(graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    lower_bounds = {}\n",
    "\n",
    "    k = 2\n",
    "    max_iterations = 1000  # 设置最大迭代次数\n",
    "\n",
    "    Y = {}\n",
    "    S_un = {}\n",
    "    nVisited = {}\n",
    "    finished = {}\n",
    "\n",
    "    for s in nodes:\n",
    "        degree_s = graph.degree(s)\n",
    "        Y[(k - 1, s)] = degree_s\n",
    "        S_un[s] = degree_s\n",
    "        nVisited[s] = degree_s + 1\n",
    "        finished[s] = False\n",
    "\n",
    "    nFinished = 0\n",
    "    while nFinished < n and k <= max_iterations:\n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "\n",
    "            if k == 2:\n",
    "                Y[(k, s)] = sum(Y[(k - 1, w)] for w in graph.neighbors(s)) - graph.degree(s)\n",
    "            elif k > 2:\n",
    "                Y[(k, s)] = sum(Y[(k - 1, w)] for w in graph.neighbors(s)) - Y.get((k - 2, s), 0) * (graph.degree(s) - 1)\n",
    "            else:\n",
    "                Y[(k, s)] = 0\n",
    "\n",
    "            nVisited[s] += Y.get((k - 1, s), 0)\n",
    "\n",
    "            S_un[s] = S_un[s] + k * Y.get((k - 1, s), 0)\n",
    "\n",
    "            if nVisited[s] >= n:\n",
    "                finished[s] = True\n",
    "                nFinished += 1\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    for v in nodes:\n",
    "        lower_bounds[v] = S_un[v]/ (n-1)\n",
    "\n",
    "    return lower_bounds\n",
    "\n",
    "\n",
    "def updateBoundsLB(graph, s):\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    d = nx.single_source_shortest_path_length(graph, s)\n",
    "    maxD = max(d.values())\n",
    "    sumL = {i: 0 for i in range(maxD+2)}\n",
    "    sumG = {i: 0 for i in range(maxD+2)}\n",
    "\n",
    "    Gamma = {i: [] for i in range(maxD + 1)}\n",
    "    for v, dist in d.items():\n",
    "        Gamma[dist].append(v)\n",
    "    sumG[maxD+1] = 0\n",
    "    for i in range(maxD):\n",
    "        sumL[i+1] = sumL[i] + len(Gamma[i+1])\n",
    "        sumG[i+1] = n - sumL[i+1]\n",
    "\n",
    "    L = {}\n",
    "    L[1] = len(Gamma[1]) + len(Gamma[2]) + sumG[2] - 2\n",
    "\n",
    "    for i in range(2, maxD+1):\n",
    "        L[i] = L[i-1] + sumL[i-2] - sumG[i+1]\n",
    "\n",
    "    LB_s = {}\n",
    "    for i in range(1, maxD+1):\n",
    "        for v in Gamma[i]:\n",
    "            LB_s[v] = (L[i] - graph.degree(v))/ (n-1)\n",
    "\n",
    "    return LB_s\n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=20):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "\n",
    "    # 算法 1 实现\n",
    "    L = neighborhood_lower_bound(graph)\n",
    "\n",
    "    k_lowest_farness = heapq.nsmallest(top_k, L.items(), key=lambda item: item[1])\n",
    "    print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    for node, farness in k_lowest_farness:\n",
    "        print(f\"Node: {node}, Farness: {farness}\")\n",
    "\n",
    "    negative_farness_nodes = {node: farness for node, farness in L.items() if farness < 0}\n",
    "    if negative_farness_nodes:\n",
    "        print(\"\\nNodes with negative farness values:\")\n",
    "        for node, farness in negative_farness_nodes.items():\n",
    "            print(f\"Node: {node}, Farness: {farness}\")\n",
    "    else:\n",
    "        print(\"\\nNo nodes with negative farness values found.\")\n",
    "        \n",
    "    Q = [(L[v], v) for v in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "    Top = []\n",
    "    Farm = L.copy()\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "    while Q and len(Top) < top_k:\n",
    "        #\n",
    "        if len(Top) >= top_k and Farm[v] > Farm[Top[-1]]:\n",
    "            break\n",
    "        \n",
    "        Top.append(v)\n",
    "        Top.sort(key=lambda x: Farm[x], reverse=True)\n",
    "\n",
    "        negative_farness_nodes_after_update = {node: farness for node, farness in Farm.items() if farness < 0}\n",
    "        if negative_farness_nodes_after_update:\n",
    "            print(\"\\nNodes with negative farness values (after update):\")\n",
    "            for node, farness in negative_farness_nodes_after_update.items():\n",
    "                print(f\"Node: {node}, Farness: {farness}\")\n",
    "        else:\n",
    "            print(\"\\nNo nodes with negative farness values found (after update).\")\n",
    "\n",
    "        # 更新 Q：更新 Farm 后重新堆化\n",
    "        Q = [(Farm[u], u) for u in graph.nodes() if u not in Top]\n",
    "        heapq.heapify(Q)\n",
    "        v_1 = v \n",
    "        _, v = heapq.heappop(Q)\n",
    "        refined_bounds = updateBoundsLB(graph, v_1)\n",
    "        Farm[v] = refined_bounds[v]\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "    \n",
    "    print(f\"\\nTop {top_k} nodes:\")\n",
    "    Cen = {}\n",
    "    for i, node in enumerate(Top[:top_k], 1):\n",
    "        #Cen[node] = 1/Farm[node]\n",
    "        #print(f\"{i}. Node: {node}, Refined Lower Centrality: {Cen[node]:.10f}\")\n",
    "        print(f\"{i}. Node: {node}, Refined Lower Centrality: {Farm[node]:.10f}\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"medium.tsv\"  # 替换为你文件的路径\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dbab5ca6-9b67-43cc-bab8-b5d5cd452361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Undirected graph created with 2158 nodes and 5136 edges.\n",
      "Calculating initial lower bounds...\n",
      "\n",
      "20 nodes with lowest farness values:\n",
      "Node: 575, Farness: 3.001390820584145\n",
      "Node: 374, Farness: 3.008344923504868\n",
      "Node: 851, Farness: 3.0139082058414464\n",
      "Node: 222, Farness: 3.044506258692629\n",
      "Node: 46, Farness: 3.097357440890125\n",
      "Node: 588, Farness: 3.09874826147427\n",
      "Node: 682, Farness: 3.2962447844228095\n",
      "Node: 628, Farness: 3.305980528511822\n",
      "Node: 529, Farness: 3.3310152990264257\n",
      "Node: 245, Farness: 3.388038942976356\n",
      "Node: 817, Farness: 3.4089012517385258\n",
      "Node: 770, Farness: 3.4255910987482614\n",
      "Node: 118, Farness: 3.49095966620306\n",
      "Node: 116, Farness: 3.4937413073713492\n",
      "Node: 717, Farness: 3.5076495132127956\n",
      "Node: 51, Farness: 3.5424200278164117\n",
      "Node: 22, Farness: 3.556328233657858\n",
      "Node: 572, Farness: 3.603616133518776\n",
      "Node: 270, Farness: 3.631432545201669\n",
      "Node: 611, Farness: 3.6481223922114046\n",
      "Processing node 575\n",
      "Refined bound for node 575: 2.477051460361613\n",
      "Added node 575 to Top. Current Top: [575]\n",
      "Processing node 374\n",
      "Refined bound for node 374: 2.4214186369958277\n",
      "Added node 374 to Top. Current Top: [374, 575]\n",
      "Processing node 851\n",
      "Refined bound for node 851: 2.5665275846082523\n",
      "Added node 851 to Top. Current Top: [374, 575, 851]\n",
      "Processing node 222\n",
      "Refined bound for node 222: 2.422809457579972\n",
      "Added node 222 to Top. Current Top: [374, 222, 575, 851]\n",
      "Processing node 46\n",
      "Refined bound for node 46: 2.364394993045897\n",
      "Added node 46 to Top. Current Top: [46, 374, 222, 575, 851]\n",
      "Processing node 588\n",
      "Refined bound for node 588: 2.446453407510431\n",
      "Added node 588 to Top. Current Top: [46, 374, 222, 588, 575, 851]\n",
      "Processing node 682\n",
      "Refined bound for node 682: 2.483541956420955\n",
      "Added node 682 to Top. Current Top: [46, 374, 222, 588, 575, 682, 851]\n",
      "Processing node 628\n",
      "Refined bound for node 628: 2.4455261937876682\n",
      "Added node 628 to Top. Current Top: [46, 374, 222, 628, 588, 575, 682, 851]\n",
      "Processing node 529\n",
      "Refined bound for node 529: 2.4390356977283263\n",
      "Added node 529 to Top. Current Top: [46, 374, 222, 529, 628, 588, 575, 682, 851]\n",
      "Processing node 245\n",
      "Refined bound for node 245: 2.3908205841446453\n",
      "Added node 245 to Top. Current Top: [46, 245, 374, 222, 529, 628, 588, 575, 682, 851]\n",
      "Processing node 817\n",
      "Refined bound for node 817: 2.4900324524802966\n",
      "Added node 817 to Top. Current Top: [46, 245, 374, 222, 529, 628, 588, 575, 682, 817, 851]\n",
      "Processing node 770\n",
      "Refined bound for node 770: 2.4306907742234585\n",
      "Added node 770 to Top. Current Top: [46, 245, 374, 222, 770, 529, 628, 588, 575, 682, 817, 851]\n",
      "Processing node 118\n",
      "Refined bound for node 118: 2.3690310616597126\n",
      "Added node 118 to Top. Current Top: [46, 118, 245, 374, 222, 770, 529, 628, 588, 575, 682, 817, 851]\n",
      "Processing node 116\n",
      "Refined bound for node 116: 2.3815484469170145\n",
      "Added node 116 to Top. Current Top: [46, 118, 116, 245, 374, 222, 770, 529, 628, 588, 575, 682, 817, 851]\n",
      "Processing node 717\n",
      "Refined bound for node 717: 2.4853963838664814\n",
      "Added node 717 to Top. Current Top: [46, 118, 116, 245, 374, 222, 770, 529, 628, 588, 575, 682, 717, 817, 851]\n",
      "Processing node 51\n",
      "Refined bound for node 51: 2.284191006026889\n",
      "Added node 51 to Top. Current Top: [51, 46, 118, 116, 245, 374, 222, 770, 529, 628, 588, 575, 682, 717, 817, 851]\n",
      "Processing node 22\n",
      "Refined bound for node 22: 2.323597589244321\n",
      "Added node 22 to Top. Current Top: [51, 22, 46, 118, 116, 245, 374, 222, 770, 529, 628, 588, 575, 682, 717, 817, 851]\n",
      "Processing node 572\n",
      "Refined bound for node 572: 2.4084376448771443\n",
      "Added node 572 to Top. Current Top: [51, 22, 46, 118, 116, 245, 572, 374, 222, 770, 529, 628, 588, 575, 682, 717, 817, 851]\n",
      "Processing node 270\n",
      "Refined bound for node 270: 2.3245248029670837\n",
      "Added node 270 to Top. Current Top: [51, 22, 270, 46, 118, 116, 245, 572, 374, 222, 770, 529, 628, 588, 575, 682, 717, 817, 851]\n",
      "Processing node 611\n",
      "Refined bound for node 611: 2.4186369958275384\n",
      "Added node 611 to Top. Current Top: [51, 22, 270, 46, 118, 116, 245, 572, 611, 374, 222, 770, 529, 628, 588, 575, 682, 717, 817, 851]\n",
      "Processing node 618\n",
      "Refined bound for node 618: 2.414000927213723\n",
      "Added node 618 to Top and trimmed. Current Top: [51, 22, 270, 46, 118, 116, 245, 572, 618, 611, 374, 222, 770, 529, 628, 588, 575, 682, 717, 817]\n",
      "Processing node 536\n",
      "Refined bound for node 536: 2.340751043115438\n",
      "Added node 536 to Top and trimmed. Current Top: [51, 22, 270, 536, 46, 118, 116, 245, 572, 618, 611, 374, 222, 770, 529, 628, 588, 575, 682, 717]\n",
      "Processing node 744\n",
      "Refined bound for node 744: 2.484469170143718\n",
      "Added node 744 to Top and trimmed. Current Top: [51, 22, 270, 536, 46, 118, 116, 245, 572, 618, 611, 374, 222, 770, 529, 628, 588, 575, 682, 744]\n",
      "Processing node 683\n",
      "Refined bound for node 683: 2.3245248029670837\n",
      "Added node 683 to Top and trimmed. Current Top: [51, 22, 270, 683, 536, 46, 118, 116, 245, 572, 618, 611, 374, 222, 770, 529, 628, 588, 575, 682]\n",
      "Processing node 732\n",
      "Refined bound for node 732: 2.4719517848864165\n",
      "Added node 732 to Top and trimmed. Current Top: [51, 22, 270, 683, 536, 46, 118, 116, 245, 572, 618, 611, 374, 222, 770, 529, 628, 588, 732, 575]\n",
      "Processing node 2032\n",
      "Refined bound for node 2032: inf\n",
      "3.977746870653686\n",
      "2.477051460361613\n",
      "Remaining nodes in Q cannot improve Top. Stopping.\n",
      "\n",
      "Final Top 10 nodes by refined lower centrality bound:\n",
      "1. Node: 51, Refined Lower Centrality: 2.2841910060\n",
      "2. Node: 22, Refined Lower Centrality: 2.3235975892\n",
      "3. Node: 270, Refined Lower Centrality: 2.3245248030\n",
      "4. Node: 683, Refined Lower Centrality: 2.3245248030\n",
      "5. Node: 536, Refined Lower Centrality: 2.3407510431\n",
      "6. Node: 46, Refined Lower Centrality: 2.3643949930\n",
      "7. Node: 118, Refined Lower Centrality: 2.3690310617\n",
      "8. Node: 116, Refined Lower Centrality: 2.3815484469\n",
      "9. Node: 245, Refined Lower Centrality: 2.3908205841\n",
      "10. Node: 572, Refined Lower Centrality: 2.4084376449\n",
      "11. Node: 618, Refined Lower Centrality: 2.4140009272\n",
      "12. Node: 611, Refined Lower Centrality: 2.4186369958\n",
      "13. Node: 374, Refined Lower Centrality: 2.4214186370\n",
      "14. Node: 222, Refined Lower Centrality: 2.4228094576\n",
      "15. Node: 770, Refined Lower Centrality: 2.4306907742\n",
      "16. Node: 529, Refined Lower Centrality: 2.4390356977\n",
      "17. Node: 628, Refined Lower Centrality: 2.4455261938\n",
      "18. Node: 588, Refined Lower Centrality: 2.4464534075\n",
      "19. Node: 732, Refined Lower Centrality: 2.4719517849\n",
      "20. Node: 575, Refined Lower Centrality: 2.4770514604\n",
      "\n",
      "Total time taken: 0.2359 seconds\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def neighborhood_lower_bound(graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    lower_bounds = {}\n",
    "\n",
    "    k = 2\n",
    "    max_iterations = 1000  # 设置最大迭代次数\n",
    "\n",
    "    Y = {}\n",
    "    S_un = {}\n",
    "    nVisited = {}\n",
    "    finished = {}\n",
    "\n",
    "    for s in nodes:\n",
    "        degree_s = graph.degree(s)\n",
    "        Y[(k - 1, s)] = degree_s\n",
    "        S_un[s] = degree_s\n",
    "        nVisited[s] = degree_s + 1\n",
    "        finished[s] = False\n",
    "\n",
    "    nFinished = 0\n",
    "    while nFinished < n and k <= max_iterations:\n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "\n",
    "            if k == 2:\n",
    "                Y[(k, s)] = sum(Y[(k - 1, w)] for w in graph.neighbors(s)) - graph.degree(s)\n",
    "            elif k > 2:\n",
    "                Y[(k, s)] = sum(Y[(k - 1, w)] for w in graph.neighbors(s)) - Y.get((k - 2, s), 0) * (graph.degree(s) - 1)\n",
    "            else:\n",
    "                Y[(k, s)] = 0\n",
    "\n",
    "            nVisited[s] += Y.get((k - 1, s), 0)\n",
    "\n",
    "            S_un[s] = S_un[s] + k * Y.get((k - 1, s), 0)\n",
    "\n",
    "            if nVisited[s] >= n:\n",
    "                finished[s] = True\n",
    "                nFinished += 1\n",
    "\n",
    "        k += 1\n",
    "\n",
    "    for v in nodes:\n",
    "        lower_bounds[v] = S_un[v]/ (n-1)\n",
    "\n",
    "    return lower_bounds\n",
    "\n",
    "\n",
    "def updateBoundsBFSCut(v, graph, x):\n",
    "    \"\"\"\n",
    "    计算将顶点 v 分隔开的割集大小的下界。\n",
    "\n",
    "    参数：\n",
    "        v: 起始顶点。\n",
    "        graph: networkx DiGraph。节点必须具有 'r' 属性。\n",
    "        Farn: 存储下界的字典（必须初始化）。\n",
    "        Top: top k 节点的列表（必须初始化）。\n",
    "        x: 一个阈值。\n",
    "\n",
    "    返回：\n",
    "        如果割集值超过 x，则返回 +∞；否则返回计算出的割集值，或者如果无变化则返回当前 Farn[v]。\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    Q = [(0,v)] #优先级队列，用于跟踪 BFS 的 (距离, 节点)\n",
    "    heapq.heapify(Q)\n",
    "    visited = {v}\n",
    "    d = 0\n",
    "    S = 0\n",
    "    y = graph.degree(v) - 1\n",
    "    nd = 1\n",
    "\n",
    "    while Q:\n",
    "        dist, u = heapq.heappop(Q)\n",
    "        if dist > d:\n",
    "            d += 1\n",
    "            #LCUT 计算\n",
    "            \n",
    "            LCUT = ((d+2)*(n-nd) + S - y )/(n-1)\n",
    "        \n",
    "\n",
    "            if LCUT >= x:\n",
    "                return float('inf')\n",
    "            y = 0  # 重置 y\n",
    "\n",
    "        for w in graph.neighbors(u):\n",
    "            if w not in visited:\n",
    "                visited.add(w)\n",
    "                dist_vw = nx.shortest_path_length(graph,v,w) #假设存在最短路径\n",
    "                heapq.heappush(Q,(dist_vw,w))\n",
    "                S += dist_vw  # 距离 d(v,w)\n",
    "                y += graph.degree(w)\n",
    "                nd += 1\n",
    "            else:\n",
    "                LCUT = LCUT + 1/(n-1)\n",
    "\n",
    "    #最终计算\n",
    "    LCUT_final = S / (n-1)\n",
    "\n",
    "    \n",
    "    return LCUT_final\n",
    "\n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=20):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds...\")\n",
    "\n",
    "    initial_lower_bounds = neighborhood_lower_bound(graph)\n",
    "    \n",
    "    k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    for node, farness in k_lowest_farness:\n",
    "        print(f\"Node: {node}, Farness: {farness}\")\n",
    "    \n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "    \n",
    "    print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "    print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        \n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # 保持Top的长度为top_k\n",
    "            print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        else:\n",
    "            print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "    \n",
    "        print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "        print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "        \n",
    "        if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "            print(Farn[Q[0][1]])\n",
    "            print(Farn[Top[-1]])\n",
    "            print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFinal Top 10 nodes by refined lower centrality bound:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        print(f\"{i}. Node: {node}, Refined Lower Centrality: {Farn[node]:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"medium.tsv\"  # 替换为你文件的路径\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aae368c7-d915-4dd3-a4d7-fce70eb12f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Undirected graph created with 2158 nodes and 5136 edges.\n",
      "Calculating initial lower bounds...\n",
      "\n",
      "10 nodes with lowest farness values:\n",
      "Node: 2, Farness: 2.289290681502086\n",
      "Node: 548, Farness: 2.611961057023644\n",
      "Node: 691, Farness: 2.634214186369958\n",
      "Node: 50, Farness: 2.66759388038943\n",
      "Node: 44, Farness: 2.6689847009735743\n",
      "Node: 565, Farness: 2.674547983310153\n",
      "Node: 150, Farness: 2.6926286509040334\n",
      "Node: 47, Farness: 2.7037552155771905\n",
      "Node: 613, Farness: 2.7357440890125173\n",
      "Node: 54, Farness: 2.7482614742698193\n",
      "Processing node 2\n",
      "Refined bound for node 2: 2.1010662957811777\n",
      "Added node 2 to Top. Current Top: [2]\n",
      "Processing node 548\n",
      "Refined bound for node 548: 2.247102457116365\n",
      "Added node 548 to Top. Current Top: [2, 548]\n",
      "Processing node 691\n",
      "Refined bound for node 691: 2.246175243393602\n",
      "Added node 691 to Top. Current Top: [2, 691, 548]\n",
      "Processing node 50\n",
      "Refined bound for node 50: 1.9573481687528975\n",
      "Added node 50 to Top. Current Top: [50, 2, 691, 548]\n",
      "Processing node 44\n",
      "Refined bound for node 44: 1.9564209550301344\n",
      "Added node 44 to Top. Current Top: [44, 50, 2, 691, 548]\n",
      "Processing node 565\n",
      "Refined bound for node 565: 2.3106165971256374\n",
      "Added node 565 to Top. Current Top: [44, 50, 2, 691, 548, 565]\n",
      "Processing node 150\n",
      "Refined bound for node 150: 2.2452480296708393\n",
      "Added node 150 to Top. Current Top: [44, 50, 2, 150, 691, 548, 565]\n",
      "Processing node 47\n",
      "Refined bound for node 47: 2.1910060268891978\n",
      "Added node 47 to Top. Current Top: [44, 50, 2, 47, 150, 691, 548, 565]\n",
      "Processing node 613\n",
      "Refined bound for node 613: 2.3291608715808994\n",
      "Added node 613 to Top. Current Top: [44, 50, 2, 47, 150, 691, 548, 565, 613]\n",
      "Processing node 54\n",
      "Refined bound for node 54: 2.1650440426518314\n",
      "Added node 54 to Top. Current Top: [44, 50, 2, 54, 47, 150, 691, 548, 565, 613]\n",
      "Processing node 716\n",
      "Refined bound for node 716: 2.2512749188687993\n",
      "Added node 716 to Top and trimmed. Current Top: [44, 50, 2, 54, 47, 150, 691, 548, 716, 565]\n",
      "Processing node 136\n",
      "Refined bound for node 136: 2.229949003245248\n",
      "Added node 136 to Top and trimmed. Current Top: [44, 50, 2, 54, 47, 136, 150, 691, 548, 716]\n",
      "Processing node 712\n",
      "Refined bound for node 712: 2.3208159480760315\n",
      "2.787204450625869\n",
      "2.2512749188687993\n",
      "Remaining nodes in Q cannot improve Top. Stopping.\n",
      "\n",
      "Final Top 10 nodes by refined lower centrality bound:\n",
      "1. Node: 44, Refined Lower Centrality: 1.9564209550\n",
      "2. Node: 50, Refined Lower Centrality: 1.9573481688\n",
      "3. Node: 2, Refined Lower Centrality: 2.1010662958\n",
      "4. Node: 54, Refined Lower Centrality: 2.1650440427\n",
      "5. Node: 47, Refined Lower Centrality: 2.1910060269\n",
      "6. Node: 136, Refined Lower Centrality: 2.2299490032\n",
      "7. Node: 150, Refined Lower Centrality: 2.2452480297\n",
      "8. Node: 691, Refined Lower Centrality: 2.2461752434\n",
      "9. Node: 548, Refined Lower Centrality: 2.2471024571\n",
      "10. Node: 716, Refined Lower Centrality: 2.2512749189\n",
      "\n",
      "Total time taken: 0.3398 seconds\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def neighborhood_lower_bound(graph):\n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    lower_bounds = {}\n",
    "\n",
    "    k = 2\n",
    "    max_iterations = 100000  # 设置最大迭代次数\n",
    "\n",
    "    Y = {}\n",
    "    S_un = {}\n",
    "    nVisited = {}\n",
    "    finished = {}\n",
    "\n",
    "    for s in nodes:\n",
    "        degree_s = graph.degree(s)\n",
    "        Y[(1, s)] = degree_s\n",
    "        S_un[s] = degree_s\n",
    "        nVisited[s] = degree_s + 1\n",
    "        finished[s] = False\n",
    "\n",
    "    nFinished = 0\n",
    "\n",
    "    while nFinished < n and k <= max_iterations:\n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "            if k == 2:\n",
    "                Y[(k, s)] = sum(Y.get((k-1, w), 0) for w in graph.neighbors(s)) - graph.degree(s)\n",
    "            else:\n",
    "                Y[(k, s)] = sum(Y.get((k-1, w), 0) for w in graph.neighbors(s)) - Y.get((k-2, s), 0) * (graph.degree(s) - 1)\n",
    "        \n",
    "        for s in nodes:\n",
    "            if finished[s]:\n",
    "                continue\n",
    "            y_k_minus_2 = Y.get((k-2, s), 0)\n",
    "            y_k_minus_1 = Y.get((k-1, s), 0)\n",
    "            nVisited[s] += y_k_minus_1\n",
    "            \n",
    "            if nVisited[s] < n:\n",
    "                S_un[s] += k * y_k_minus_1\n",
    "            else:\n",
    "                S_un[s] += k *(n-(nVisited[s] - y_k_minus_1))\n",
    "                nFinished += 1\n",
    "                finished[s] = True\n",
    "            Y[(k-2, s)] = y_k_minus_1\n",
    "            Y[(k-1, s)] = Y[(k, s)]\n",
    "        \n",
    "        k += 1\n",
    "\n",
    "    for v in nodes:\n",
    "        lower_bounds[v] = S_un[v] / (n - 1)\n",
    "\n",
    "    return lower_bounds\n",
    "\n",
    "def updateBoundsBFSCut(v, graph, x):\n",
    "    \"\"\"\n",
    "    计算将顶点 v 分隔开的割集大小的下界。\n",
    "\n",
    "    参数：\n",
    "        v: 起始顶点。\n",
    "        graph: networkx DiGraph。节点必须具有 'r' 属性。\n",
    "        Farn: 存储下界的字典（必须初始化）。\n",
    "        Top: top k 节点的列表（必须初始化）。\n",
    "        x: 一个阈值。\n",
    "\n",
    "    返回：\n",
    "        如果割集值超过 x，则返回 +∞；否则返回计算出的割集值，或者如果无变化则返回当前 Farn[v]。\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    Q = [(0,v)] #优先级队列，用于跟踪 BFS 的 (距离, 节点)\n",
    "    heapq.heapify(Q)\n",
    "    visited = {v}\n",
    "    d = 0\n",
    "    S = 0\n",
    "    y = graph.degree(v) - 1\n",
    "    nd = 1\n",
    "\n",
    "    while Q:\n",
    "        dist, u = heapq.heappop(Q)\n",
    "        if dist > d:\n",
    "            d += 1\n",
    "            #LCUT 计算\n",
    "            \n",
    "            LCUT = ((d+2)*(n-nd) + S - y )/(n-1)\n",
    "        \n",
    "\n",
    "            if LCUT >= x:\n",
    "                return float('inf')\n",
    "            y = 0  # 重置 y\n",
    "\n",
    "        for w in graph.neighbors(u):\n",
    "            if w not in visited:\n",
    "                visited.add(w)\n",
    "                dist_vw = nx.shortest_path_length(graph,v,w) #假设存在最短路径\n",
    "                heapq.heappush(Q,(dist_vw,w))\n",
    "                S += dist_vw  # 距离 d(v,w)\n",
    "                y += graph.degree(w)\n",
    "                nd += 1\n",
    "            else:\n",
    "                LCUT = LCUT + 1/(n-1)\n",
    "\n",
    "    #最终计算\n",
    "    LCUT_final = S / (n-1)\n",
    "\n",
    "    \n",
    "    return LCUT_final\n",
    "\n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds...\")\n",
    "\n",
    "    initial_lower_bounds = neighborhood_lower_bound(graph)\n",
    "    \n",
    "    k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    for node, farness in k_lowest_farness:\n",
    "        print(f\"Node: {node}, Farness: {farness}\")\n",
    "    \n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "    \n",
    "    print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "    print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        \n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # 保持Top的长度为top_k\n",
    "            print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        else:\n",
    "            print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "    \n",
    "        print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "        print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "        \n",
    "        if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "            print(Farn[Q[0][1]])\n",
    "            print(Farn[Top[-1]])\n",
    "            print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFinal Top 10 nodes by refined lower centrality bound:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        print(f\"{i}. Node: {node}, Refined Lower Centrality: {Farn[node]:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"com-amazon.ungraph.tsv\"  # 替换为你文件的路径\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1df742c3-dec0-4d16-9197-c505a3777bcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Undirected graph created with 17560 nodes and 22732 edges.\n",
      "Calculating top k nodes by closeness centrality...\n",
      "\n",
      "Top 10 nodes by closeness centrality:\n",
      "1. Node: 363, Closeness Centrality: 0.4734543101\n",
      "2. Node: 106, Closeness Centrality: 0.4641186266\n",
      "3. Node: 18, Closeness Centrality: 0.4505542441\n",
      "4. Node: 4, Closeness Centrality: 0.4431293375\n",
      "5. Node: 104, Closeness Centrality: 0.4408596751\n",
      "6. Node: 78, Closeness Centrality: 0.4394144144\n",
      "7. Node: 210, Closeness Centrality: 0.4315523004\n",
      "8. Node: 730, Closeness Centrality: 0.4310861239\n",
      "9. Node: 404, Closeness Centrality: 0.4255386181\n",
      "10. Node: 91, Closeness Centrality: 0.4192092823\n",
      "\n",
      "Total time taken: 33.0521 seconds\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def calculate_closeness_centrality(graph, top_k=10):\n",
    "    \"\"\"\n",
    "    计算图中节点的接近中心性,并返回 top k 个最高中心性的节点。\n",
    "\n",
    "    参数:\n",
    "    graph (networkx.Graph): 输入图\n",
    "    top_k (int): 返回前 k 个最高中心性的节点\n",
    "\n",
    "    返回:\n",
    "    dict: 包含 top k 个节点及其接近中心性值的字典\n",
    "    \"\"\"\n",
    "    # 计算每个节点的接近中心性\n",
    "    closeness_centrality = nx.closeness_centrality(graph)\n",
    "\n",
    "    # 获取 top k 个最高中心性的节点\n",
    "    top_nodes = heapq.nlargest(top_k, closeness_centrality, key=closeness_centrality.get)\n",
    "    top_centrality = {node: closeness_centrality[node] for node in top_nodes}\n",
    "\n",
    "    return top_centrality\n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "\n",
    "    print(\"Calculating top k nodes by closeness centrality...\")\n",
    "    top_closeness = calculate_closeness_centrality(graph, top_k)\n",
    "\n",
    "    print(\"\\nTop 10 nodes by closeness centrality:\")\n",
    "    for i, (node, centrality) in enumerate(top_closeness.items(), 1):\n",
    "        print(f\"{i}. Node: {node}, Closeness Centrality: {centrality:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"com-youtube.ungraph.tsv\"  # 替换为你文件的路径\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "16bebb53-ad1a-407b-b4c3-3623a90839cb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating graph...\n",
      "Undirected graph created with 17560 nodes and 22732 edges.\n",
      "Calculating initial lower bounds using PageRank...\n",
      "\n",
      "10 nodes with lowest farness values:\n",
      "Node: 106, Farness: 16.483694625298675\n",
      "Node: 4, Farness: 17.24106987697447\n",
      "Node: 104, Farness: 18.269346717532212\n",
      "Node: 22, Farness: 23.126325862788764\n",
      "Node: 78, Farness: 40.40284292522227\n",
      "Node: 34, Farness: 44.21001879512881\n",
      "Node: 72, Farness: 55.64863839124198\n",
      "Node: 47, Farness: 60.82988337035862\n",
      "Node: 33, Farness: 67.33797845220953\n",
      "Node: 91, Farness: 87.623296934277\n",
      "Processing node 106\n",
      "Refined bound for node 106: 2.1546215615923456\n",
      "Added node 106 to Top. Current Top: [106]\n",
      "Processing node 4\n",
      "Refined bound for node 4: 2.2566774873284356\n",
      "Added node 4 to Top. Current Top: [106, 4]\n",
      "Processing node 104\n",
      "Refined bound for node 104: 2.2682954610171424\n",
      "Added node 104 to Top. Current Top: [106, 4, 104]\n",
      "Processing node 22\n",
      "Refined bound for node 22: 2.688194088501623\n",
      "Added node 22 to Top. Current Top: [106, 4, 104, 22]\n",
      "Processing node 78\n",
      "Refined bound for node 78: 2.275756022552537\n",
      "Added node 78 to Top. Current Top: [106, 4, 104, 78, 22]\n",
      "Processing node 34\n",
      "Refined bound for node 34: 2.8107523207471954\n",
      "Added node 34 to Top. Current Top: [106, 4, 104, 78, 22, 34]\n",
      "Processing node 72\n",
      "Refined bound for node 72: 2.8307990204453555\n",
      "Added node 72 to Top. Current Top: [106, 4, 104, 78, 22, 34, 72]\n",
      "Processing node 47\n",
      "Refined bound for node 47: 2.528503901133322\n",
      "Added node 47 to Top. Current Top: [106, 4, 104, 78, 47, 22, 34, 72]\n",
      "Processing node 33\n",
      "Refined bound for node 33: 2.9152571330941397\n",
      "Added node 33 to Top. Current Top: [106, 4, 104, 78, 47, 22, 34, 72, 33]\n",
      "Processing node 91\n",
      "Refined bound for node 91: 2.3854433623782674\n",
      "Added node 91 to Top. Current Top: [106, 4, 104, 78, 91, 47, 22, 34, 72, 33]\n",
      "Processing node 94\n",
      "Refined bound for node 94: 2.575602255253716\n",
      "Added node 94 to Top and trimmed. Current Top: [106, 4, 104, 78, 91, 47, 94, 22, 34, 72]\n",
      "Processing node 45\n",
      "Refined bound for node 45: 2.3997380260834897\n",
      "Added node 45 to Top and trimmed. Current Top: [106, 4, 104, 78, 91, 45, 47, 94, 22, 34]\n",
      "Processing node 18\n",
      "Refined bound for node 18: 2.219488581354291\n",
      "Added node 18 to Top and trimmed. Current Top: [106, 18, 4, 104, 78, 91, 45, 47, 94, 22]\n",
      "Processing node 97\n",
      "Refined bound for node 97: inf\n",
      "inf\n",
      "2.688194088501623\n",
      "Remaining nodes in Q cannot improve Top. Stopping.\n",
      "\n",
      "Final Top 10 nodes by refined lower centrality bound:\n",
      "1. Node: 106, Refined Lower Centrality: 2.1546215616\n",
      "2. Node: 18, Refined Lower Centrality: 2.2194885814\n",
      "3. Node: 4, Refined Lower Centrality: 2.2566774873\n",
      "4. Node: 104, Refined Lower Centrality: 2.2682954610\n",
      "5. Node: 78, Refined Lower Centrality: 2.2757560226\n",
      "6. Node: 91, Refined Lower Centrality: 2.3854433624\n",
      "7. Node: 45, Refined Lower Centrality: 2.3997380261\n",
      "8. Node: 47, Refined Lower Centrality: 2.5285039011\n",
      "9. Node: 94, Refined Lower Centrality: 2.5756022553\n",
      "10. Node: 22, Refined Lower Centrality: 2.6881940885\n",
      "\n",
      "Total time taken: 12.8615 seconds\n"
     ]
    }
   ],
   "source": [
    "import heapq\n",
    "import csv\n",
    "import re\n",
    "import time\n",
    "import cProfile\n",
    "from collections import defaultdict\n",
    "import networkx as nx\n",
    "import itertools\n",
    "import pandas as pd\n",
    "\n",
    "def pagerank_lower_bound(graph):\n",
    "    # 计算每个节点的 PageRank 值\n",
    "    page_ranks = nx.pagerank(graph)\n",
    "    \n",
    "    # 将 PageRank 值的倒数作为下界\n",
    "    lower_bounds = {node: 1 / page_ranks[node] for node in graph.nodes()}\n",
    "    \n",
    "    return lower_bounds\n",
    "\n",
    "def updateBoundsBFSCut(v, graph, x):\n",
    "    \"\"\"\n",
    "    计算将顶点 v 分隔开的割集大小的下界。\n",
    "\n",
    "    参数：\n",
    "        v: 起始顶点。\n",
    "        graph: networkx DiGraph。节点必须具有 'r' 属性。\n",
    "        Farn: 存储下界的字典（必须初始化）。\n",
    "        Top: top k 节点的列表（必须初始化）。\n",
    "        x: 一个阈值。\n",
    "\n",
    "    返回：\n",
    "        如果割集值超过 x，则返回 +∞；否则返回计算出的割集值，或者如果无变化则返回当前 Farn[v]。\n",
    "    \"\"\"\n",
    "    \n",
    "    nodes = list(graph.nodes())\n",
    "    n = len(nodes)\n",
    "    Q = [(0,v)] #优先级队列，用于跟踪 BFS 的 (距离, 节点)\n",
    "    heapq.heapify(Q)\n",
    "    visited = {v}\n",
    "    d = 0\n",
    "    S = 0\n",
    "    y = graph.degree(v) - 1\n",
    "    nd = 1\n",
    "\n",
    "    while Q:\n",
    "        dist, u = heapq.heappop(Q)\n",
    "        if dist > d:\n",
    "            d += 1\n",
    "            #LCUT 计算\n",
    "            \n",
    "            LCUT = ((d+2)*(n-nd) + S - y )/(n-1)\n",
    "        \n",
    "\n",
    "            if LCUT >= x:\n",
    "                return float('inf')\n",
    "            y = 0  # 重置 y\n",
    "\n",
    "        for w in graph.neighbors(u):\n",
    "            if w not in visited:\n",
    "                visited.add(w)\n",
    "                dist_vw = nx.shortest_path_length(graph,v,w) #假设存在最短路径\n",
    "                heapq.heappush(Q,(dist_vw,w))\n",
    "                S += dist_vw  # 距离 d(v,w)\n",
    "                y += graph.degree(w)\n",
    "                nd += 1\n",
    "            else:\n",
    "                LCUT = LCUT + 1/(n-1)\n",
    "\n",
    "    #最终计算\n",
    "    LCUT_final = S / (n-1)\n",
    "\n",
    "    \n",
    "    return LCUT_final\n",
    "\n",
    "\n",
    "def create_mention_graph_with_centrality(filepath, top_k=10):\n",
    "    start_time = time.time()\n",
    "\n",
    "    data = pd.read_csv(filepath, sep=\"\\t\", header=None)\n",
    "    directed_graph = nx.DiGraph()\n",
    "    directed_graph.add_edges_from(data.values.tolist())\n",
    "    graph = directed_graph.to_undirected()\n",
    "\n",
    "    print(f\"Undirected graph created with {graph.number_of_nodes()} nodes and {graph.number_of_edges()} edges.\")\n",
    "    print(\"Calculating initial lower bounds using PageRank...\")\n",
    "\n",
    "    initial_lower_bounds = pagerank_lower_bound(graph)\n",
    "    \n",
    "    k_lowest_farness = heapq.nsmallest(top_k, initial_lower_bounds.items(), key=lambda item: item[1])\n",
    "    print(f\"\\n{top_k} nodes with lowest farness values:\")\n",
    "    for node, farness in k_lowest_farness:\n",
    "        print(f\"Node: {node}, Farness: {farness}\")\n",
    "    \n",
    "    Farn = initial_lower_bounds.copy()\n",
    "    Top = []\n",
    "    Q = [(Farn[node], node) for node in graph.nodes()]\n",
    "    heapq.heapify(Q)\n",
    "\n",
    "    initial_threshold = float('inf')\n",
    "\n",
    "\n",
    "    _, v = heapq.heappop(Q)\n",
    "    \n",
    "    print(f\"Processing node {v}\")\n",
    "\n",
    "    threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "    refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "    print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "    Farn[v] = refined_bound\n",
    "\n",
    "    while Q:\n",
    "        \n",
    "        if len(Top) < top_k:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            print(f\"Added node {v} to Top. Current Top: {Top}\")\n",
    "        elif refined_bound < Farn[Top[-1]]:\n",
    "            Top.append(v)\n",
    "            Top.sort(key=lambda node: Farn[node])\n",
    "            Top = Top[:top_k]  # 保持Top的长度为top_k\n",
    "            print(f\"Added node {v} to Top and trimmed. Current Top: {Top}\")\n",
    "        else:\n",
    "            print(f\"Node {v} not added to Top as its bound is not better than current top\")\n",
    "\n",
    "        _, v = heapq.heappop(Q)\n",
    "    \n",
    "        print(f\"Processing node {v}\")\n",
    "\n",
    "        threshold = initial_threshold if len(Top) < top_k else Farn[Top[-1]]\n",
    "        refined_bound = updateBoundsBFSCut(v, graph, threshold)\n",
    "    \n",
    "        print(f\"Refined bound for node {v}: {refined_bound}\")\n",
    "\n",
    "        Farn[v] = refined_bound\n",
    "        \n",
    "        if len(Top) == top_k and Farn[v] >= Farn[Top[-1]]:\n",
    "            print(Farn[v])\n",
    "            print(Farn[Top[-1]])\n",
    "            print(\"Remaining nodes in Q cannot improve Top. Stopping.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nFinal Top 10 nodes by refined lower centrality bound:\")\n",
    "    for i, node in enumerate(Top, 1):\n",
    "        print(f\"{i}. Node: {node}, Refined Lower Centrality: {Farn[node]:.10f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nTotal time taken: {total_time:.4f} seconds\")\n",
    "\n",
    "    return graph\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    filepath = \"com-youtube.ungraph.tsv\"  # 替换为你文件的路径\n",
    "    print(\"Creating graph...\")\n",
    "    graph_with_centrality = create_mention_graph_with_centrality(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77478155-78a7-4fec-beca-b649886f447e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
